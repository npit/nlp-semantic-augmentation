#!/usr/bin/env python3
import collections
import numpy as np
import sys
from os.path import join, basename, split, dirname
import pickle
from matplotlib import pyplot as plt
from sklearn import metrics

""" Does post-processing analysis for the jnle revision, ie prints confusion matrix and top instances/labels"""

outputfolder = "/home/nik/work/submissions/NLE-special/tex/confmatrices"

def get_config_name(path):
    return split(split(split(path)[0])[0])[-1]

def heatmap(arr, base_outpath, labelnames):
    # labelnames = list(range(len(labelnames)))
    confname = get_config_name(base_outpath)
    print("Using config name:", confname)
    # regular cmat
    colormap = "viridis"

    plt.figure()

    outpath = join(outputfolder, confname + ".png")

    plt.imshow(arr, cmap=colormap)
    plt.colorbar()
    plt.gca().set_xticks(range(len(labelnames)))
    plt.gca().set_xticklabels(labelnames)
    plt.gca().set_yticks(range(len(labelnames)))
    plt.gca().set_yticklabels(labelnames)
    plt.xlabel(" ")
    plt.setp(plt.gca().get_xticklabels(), rotation=90, ha="right", rotation_mode="anchor")

    # plt.axis('off')
    # plt.tick_params(
    #     axis='x',          # changes apply to the x-axis
    #     which='both',      # both major and minor ticks are affected
    #     bottom=False,      # ticks along the bottom edge are off
    #     top=False,         # ticks along the top edge are off
    #     labelbottom=False)


    plt.savefig(outpath, bbox_inches='tight', pad_inches=0)
    plt.clf()
    print("Wrote {}".format(outpath))
    # also save cmat
    with open(outpath + ".pkl", "wb") as f:
        pickle.dump(arr, f)

    # diagonal-removed cmat
    outpath = join(outputfolder, confname + "_nodiag.png")
    for i in range(len(arr)):
        arr[i, i] = 0
    print(arr.shape)
    plt.figure()
    plt.imshow(arr, cmap=colormap)
    plt.colorbar()
    plt.xlabel(" ")
    plt.gca().set_xticks(range(len(labelnames)))
    plt.gca().set_xticklabels(labelnames)
    plt.gca().set_yticks(range(len(labelnames)))
    plt.gca().set_yticklabels(labelnames)
    plt.setp(plt.gca().get_xticklabels(), rotation=90, ha="right", rotation_mode="anchor")

    plt.savefig(outpath, bbox_inches='tight', pad_inches=0)
    print("Wrote {}".format(outpath))
    # also save cmat
    with open(outpath + ".pkl", "wb") as f:
        pickle.dump(arr, f)


def minmax_instances_labels(predictions, performance, num_test_labels, configname):
    res = {"instances": [], "labels": []}
    res_nosort = {"instances": [], "labels": []}

    # instance-wise
    aggregate = np.zeros((len(test_labels),), np.float32)
    for preds in predictions:
        # get true / false predictions
        non_nan_idx = np.where(np.any(~np.isnan(preds), axis=1))
        if len(non_nan_idx[0]) != num_test_labels:
            print("num test error")
            import ipdb; ipdb.set_trace()
            exit(1)
        preds = preds[non_nan_idx]
        true_labels = test_labels[non_nan_idx]
        correct_preds = np.where(np.equal(np.argmax(preds, axis=1), true_labels))
        aggregate[correct_preds] += 1
    # average
    aggregate /= len(predictions)
    # sort scores and instance indexes
    ranked_scores_idxs = sorted(list(zip(aggregate, list(range(len(aggregate))))), key=lambda x: x[0], reverse=True)
    res["instances"] = ranked_scores_idxs
    res_nosort["instances"] = aggregate

    # label-wise
    res["labels"] = {}
    for measure in ["f1-score"]:
        scores_cw = performance['run']['f1-score']['classwise']['folds']
        scores_cw = [s[lbl_indexes] for s in scores_cw]
        # average accross folds
        scores_cw = sum(scores_cw) / len(scores_cw)
        # rank
        ranked_scores_idxs = sorted(list(zip(scores_cw, list(range(len(scores_cw))))), key=lambda x: x[0], reverse=True)
        res["labels"] = ranked_scores_idxs
        res_nosort["labels"] = scores_cw

    baseoutpath = join(outputfolder, configname + "_rankings")
    # print figures as vertical heatmaps
    for what in ["labels", "instances"]:
        plt.figure()
        plt.bar(range(len(res_nosort[what])), res_nosort[what])
        if what == "labels":
            plt.gca().set_xticks(range(len(labelnames)))
            plt.gca().set_xticklabels(labelnames)
            plt.setp(plt.gca().get_xticklabels(), rotation=90, ha="right", rotation_mode="anchor")
            plt.ylabel("f1-score")
            plt.xlabel("label")

        # if what == "instances":
        #     plt.bar(range(len(res_nosort[what])), res_nosort[what])
        #     # plt.scatter(range(len(res_nosort[what])), res_nosort[what])
        # else:
        #     plt.plot(res_nosort[what])
        outpath = "{}_{}_plot.png".format(baseoutpath, what)
        plt.savefig(outpath, bbox_inches='tight', pad_inches=0)
        print("Wrote", outpath)

        show_width = 200 if what == "instances" else 1
        expanded = np.repeat(np.expand_dims(res_nosort[what], axis=0), show_width, axis=0)
        plt.figure()
        plt.imshow(expanded)
        plt.gca().set_yticks([])
        outpath = "{}_{}_heatmap.png".format(baseoutpath, what)
        plt.savefig(outpath, bbox_inches='tight', pad_inches=0)
        print("Wrote", outpath)

    print()


    # # print
    # topk=10
    # # print in format instance1, instance2, ...
    # # print the below error / stat visualization
    # print_types = {"top": lambda x: x[:topk], "bottom": lambda x: x[-topk:]}
    # for print_type, func in print_types.items():
    #     indexes = " ".join("{:.0f}".format(x[1]) for x in func(res["instances"]))
    #     scores = " ".join("{:1.3f}".format(x[0]) for x in func(res["instances"]))
    #     print("{:10} {:7} {:10} | ({}) ({})".format("accuracy", print_type, "instances", indexes, scores))

    # lbl_top = topk
    # print_types = {"top": lambda x: x[:lbl_top], "bottom": lambda x: x[-lbl_top:]}
    # measure = "f1-score"
    # for print_type, func in print_types.items():
    #     print("{:10} {:7} {:10} | ({}) ({})".format(measure, print_type, "labels",
    #                                                     " ".join("{:.0f}".format(x[1]) for x in func(res["labels"])),
    #                                                     " ".join("{:1.3f}".format(x[0]) for x in func(res["labels"]))))



if __name__ == "__main__":
    datasets = {"20ng":    "/home/nik/software/sematext-jnle/serialization/datasets/20newsgroups.preprocessed.pickle",
                "reuters": "/home/nik/software/sematext-jnle/serialization/datasets/reuters.preprocessed.pickle",
            "bbc": "/home/nik/software/sematext-jnle/serialization/datasets/bbc.json.preprocessed.pickle",
            "ohsumed": "/home/nik/software/sematext-jnle/serialization/datasets/ohsumed.json.preprocessed.pickle"}
    sem = {"20ng":    "/home/nik/software/sematext-jnle/serialization/semantic/20newsgroups_wordnet_tfidf_all_disamfirst.preprocessed.pickle",
                "reuters": "/home/nik/software/sematext-jnle/serialization/semantic/reuters_wordnet_tfidf_all_disampos_spread3-0.6.preprocessed.pickle"}


    try:
        base_dataset = sys.argv[1]
        results_folder = sys.argv[2]
    except IndexError:
        print("arg error")
        exit(1)

    dset = datasets[base_dataset]
    with open(dset, "rb") as f:
        dataset = pickle.load(f)
        labelnames = dataset[2]

        full_test_labels = dataset[4]


    if base_dataset != "bbc":
        # labels histogram
        thresh = 20
        hist = sorted(collections.Counter(full_test_labels).items(), key=lambda x: x[1], reverse=True)
        full_labelset = sorted(set(full_test_labels))
        labelset = [x[0] for x in hist if x[1] > thresh]
        inst_indexes = [i for i in range(len(full_test_labels)) if full_test_labels[i] in labelset]
        lbl_indexes = np.asarray([i for i in range(len(full_labelset)) if full_labelset[i] in labelset])
        test_labels = full_test_labels[inst_indexes]
        # labelnames = [labelnames[x] for x in lbl_indexes]
        labelnames = lbl_indexes
    else:
        test_labels = None
        lbl_indexes = list(range(len(labelnames)))
        inst_indexes = None
        train_labels = dataset[1]
        with open(join(results_folder, "mlp_fold0.trainval.pickle"), "rb") as f:
            train_val = pickle.load(f)

    res_path = join(results_folder, "results.pickle")
    with open(res_path, "rb") as f:
        res = pickle.load(f)
    performance = res["results"]
    print("Mi/ma f1 stdev:", performance["run"]["f1-score"]["micro"]["std"], performance["run"]["f1-score"]["macro"]["std"])
    predictions = []

    try:
        cmat_folds = res["confusion"]["run"]
        print("Loaded precomputed confusion matrix")
        if len(labelset) > 20:
            cmat_folds = []
    except:
        cmat_folds = []

    # gather foldwise results
    for fold in range(5):
        # print("Fold", fold)
        preds_path = join(results_folder, "mlp_fold{}.predictions.pickle".format(fold))
        with open(preds_path,"rb") as f:
            full_preds = pickle.load(f)
        if inst_indexes is not None:
            preds = full_preds[inst_indexes]
        else:
            preds = full_preds
            import pdb;pdb.set_trace()
            test_labels = [train_labels[x] for x in train_val[fold][1]]

        # thresholding
        if len(preds) != len(test_labels):
            print("Error preds / lbls shape ", len(preds), len(test_labels))
            exit(1)

        if len(cmat_folds) < 5:
            if len(full_test_labels) == 0:
                cmat = metrics.confusion_matrix(test_labels, np.argmax(preds, axis=1))
            else:
                # generate conf. matrix on the fly
                cmat = metrics.confusion_matrix(full_test_labels, np.argmax(full_preds, axis=1))
            print("Generated confusion matrix on the fly")
            if len(test_labels) < len(full_test_labels):
                cmat = cmat[lbl_indexes, :][:, lbl_indexes]
            cmat_folds.append(cmat)


        predictions.append(preds)

    # get mean of confusion matrix
    cmat = np.mean(cmat_folds, axis=0)
    heatmap(cmat, res_path + "confusion".format(fold), labelnames)
    # get min/max instances & labels
    minmax_instances_labels(predictions, performance, len(test_labels), get_config_name(res_path))
