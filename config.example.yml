# run identifier - if not specified will be autogen'd wrt to current datetime
dataset:
  # which dataset
  name: 20newsgroups
  # limit the dataset to a number of documents for train / test: <int>
  # data_limit: 50
  # class_limit: 2

# representations
representation:
  # Which representation will be used: <pretrained_embedding_name> / train / bag / tfidf
  # Pretrained embeddings should be python pickles
  name: glove
  # for token-based representation, path to the token list to be used
  token_list:
  # representation dimension: <int>
  dimension: 100
  # Sequence length, for embedding training: <int>
  sequence_length: 10
  # aggregation method:
  # - avg: average word representations to a single doc vector
  # - [pad,NUM,filter]: e.g. [pad, 10, first]
  # -- pad / num: Pad (or prune) to NUM items: pad, <int>
  # -- filter: how to select these NUM items:
  # --- first: just get the first NUM words
  # --- freq: get the first NUM words that are most frequent in the dataset (TODO)
  #aggregation: [pad, 10, first]
  aggregation: avg
  # what to do with words missing from the embedding: drop / unk
  # unk: map to an <unknown> category
  # drop: discard them
  unknown_words: unk


transform:
  # Optional feature transformation method
  name: LSA
  dimension: 100

# semantic
semantic:
  # which semantic resource will be used. Values: wordnet, context
  name: wordnet
  # what is considered as a semantic information. Values: synset
  unit: concept
  # the form of semantic information to use. Values: frequencies / tfidf
  weights: tfidf
  # optional limit to enforce on semantic information. <num, filter>
  # filter: how to enforce the limit.
  # - first: keep the <limit> first concepts, with the highest weight
  # - frequency-threshold: keep only concepts that appear at least <num> times in the dataset
  limit: 15, first
  # how the semantic information is combined with the textual one. Values: concat, replace
  enrichment: replace

  # how candidate semantic information units are selected, given a word. Values: first, pos, context_embedding
  # first: get first retrieved unit
  # pos: get first that matches the POS tag assigned
  #disambiguation: first
  disambiguation: pos
  # spread semantic unit activation: <maxsteps>, <decay>
  # maxsteps: number of allowed jumps in the semantic graph. Jump direction is handled within each semantic resource: <int>
  # decay: decay factor of the weight assigned to each successive step: <float>
  spreading_activation: [2, 0.8]

  # parameters for context_embeddings disambiguation
  # how embeddings are combined. Values: avg
  context_aggregation: avg
  # path to pre-computed context of each semantic unit
  context_file: wordnet_synset_examples_definitions.pickle
  # context word-wise frequency threshold. semantic units with less context than this are dropped
  context_threshold: 25

# learning model
# dnn,hiddensize,numlayers
# lstm,hiddensize,numlayers
# embedding,hiddensize,numlayers
# dnn or lstm
learner:
  # Values: mlp / lstm
  # name: lstm
  name: mlp
  # Learner parameters.
  hidden_dim:  1024
  layers: 2
  sequence_length: 10
  # forbid loading existing predictions
  no_load: False



train:
  epochs: 40
  folds: 5
  validation_portion: 0.1
  early_stopping_patience: 20
  batch_size: 20

log_level: debug



print:
  # evaluation metrics: f1, accuracy, precision, recall
  measures: ["f1-score"]
  # evaluation metric aggregations: macro, micro, weighted
  aggregations: ["macro", "micro"]
  # which run type is of interest
  # run: the actual run you are evaluating
  # majority: a majority classifier baseline
  # random: a random classifier baseline
  run_types: [run]
  # stats aggregating fold values
  stats: ["mean", "var"]

folders:
  # run folder
  run: glove100_mlp
  # folder to store serialization results
  serialization: "serialization"
  # folder to supply raw data, where applicable
  raw_data: raw_data
